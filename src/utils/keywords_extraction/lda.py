from gensim.models.ldamodel import LdaModel
import numpy as np

class topic_model():
    """docstring for topic_model"""
    def __init__(self,path='523model/523.model'):
        self.lda=LdaModel.load(path)
        self.dictionary=self.lda.id2word

    def get_doc_topics(self,text):
        text=text.split()
        self.bow=self.dictionary.doc2bow(text)
        return np.array(self.lda.get_document_topics(self.bow,0))[:,1]

    def get_doc_keywords(self,text,num=3):
        #out=[]
        out={}
        topics = self.get_doc_topics(text)
        topic = topics.reshape(1,len(topics))
        words_matrix = np.zeros((len(topics),len(self.bow)))
        if len(self.bow)==0:
            return out

        for n in range(0,len(self.bow)):
            word=self.bow[n]
            word_vec=self.lda.get_term_topics(word[0],0)
            for t in word_vec:
                words_matrix[t[0]][n]=t[1]
        weight=np.dot(topic,words_matrix)
        for i in range(num):
            max=0
            for j in range(0,np.size(weight[0])):
                if weight[0][j]>weight[0][max]:
                    max=j
            if weight[0][max]==0.0:
                break
            #out.append((self.dictionary[self.bow[max][0]],weight[0][max]))
            out[self.dictionary[self.bow[max][0]]]=weight[0][max]
            weight[0][max]=0
        return out

if __name__ == '__main__':
    # mo=topic_model('521model/521.model')#å¯é€‰é¡¹ä¸ºåŠ è½½æ¨¡å‹çš„è·¯å¾„
    # #mo.lda.save('533.model')
    # #print(mo.get_doc_topics('å¯¹äº å›½å†… åŠ¨æ¼« ç”»ä½œè€… å¼•ç”¨ å·¥ç¬” ç´ æ çš„ ä¸€äº› ä¸ªäºº æ„è§ ã€‚'))#è¾“å…¥çš„æ–‡æœ¬å¿…é¡»æ˜¯å·²åˆ†è¯ï¼Œä¸”ç”¨ç©ºæ ¼éš”å¼€çš„å­—ç¬¦ä¸²
    # print(mo.get_doc_keywords('ä½  å¥½ ï¼Œä¸æ˜¯ é’¢é“ æ€æ · ç‚¼æˆ çš„ è€Œæ˜¯ é™é™çš„ é¡¿æ²³',10))#è¾“å…¥æ–‡æœ¬åŒä¸Šï¼Œå¯é€‰é¡¹ä¸ºè¦é€‰çš„å…³é”®è¯çš„æ•°é‡ï¼Œå¦‚æœè¶…è¿‡è¯è¢‹æ•°ç›®ï¼Œåˆ™ä¸è¿”å›å…³é”®è¯
    # print(mo.get_doc_keywords('å…«è·¯å†› ä¹”è£… æ”¹ æ‰®æˆ æ—¥æœ¬ å†›å®˜ åˆºæ¢ å†›æƒ…',10))#è¾“å…¥æ–‡æœ¬åŒä¸Šï¼Œå¯é€‰é¡¹ä¸ºè¦é€‰çš„å…³é”®è¯çš„æ•°é‡ï¼Œå¦‚æœè¶…è¿‡è¯è¢‹æ•°ç›®ï¼Œåˆ™ä¸è¿”å›å…³é”®è¯
    # print(mo.get_doc_keywords('æˆ‘ è¦ æ˜¯ ä»¥å å† ä¸‹åˆ å– å’–å•¡ æˆ‘ ä»–å¦ˆ å°±æ˜¯  ğŸ† ã€‚',10))#è¾“å…¥æ–‡æœ¬åŒä¸Šï¼Œå¯é€‰é¡¹ä¸ºè¦é€‰çš„å…³é”®è¯çš„æ•°é‡ï¼Œå¦‚æœè¶…è¿‡è¯è¢‹æ•°ç›®ï¼Œåˆ™ä¸è¿”å›å…³é”®è¯
    # print(mo.get_doc_keywords('çŒ®ç»™ æ‰€æœ‰ å’Œ å¥¶å¥¶ æœ‰ ç€ ç«¥å¹´ çš„ å­©å­ ã€‚',10))#è¾“å…¥æ–‡æœ¬åŒä¸Šï¼Œå¯é€‰é¡¹ä¸ºè¦é€‰çš„å…³é”®è¯çš„æ•°é‡ï¼Œå¦‚æœè¶…è¿‡è¯è¢‹æ•°ç›®ï¼Œåˆ™ä¸è¿”å›å…³é”®è¯

    import sys
    import os.path
    import jieba

    tm = topic_model(os.path.expanduser('~/.complex_qa/topic_model/topic.model'))
    for line in open(sys.argv[1]):
        sents = line.rstrip().split('\t')
        topics = []
        for sent in sents:
            text = ' '.join(jieba.cut(sent))
            kwds = tm.get_doc_keywords(text, 3)
            topics.extend(kwds.keys())
        topics = list(set(topics))
        print('\t'.join(sents + [','.join(topics)]))
